
% VLDB template version of 2020-08-03 enhances the ACM template, version 1.7.0:
% https://www.acm.org/publications/proceedings-template
% The ACM Latex guide provides further information about the ACM template

\documentclass[sigconf, nonacm]{acmart}

%% The following content must be adapted for the final version
% paper-specific
\newcommand\vldbdoi{XX.XX/XXX.XX}
\newcommand\vldbpages{XXX-XXX}
% issue-specific
\newcommand\vldbvolume{14}
\newcommand\vldbissue{1}
\newcommand\vldbyear{2020}
% should be fine as it is
\newcommand\vldbauthors{\authors}
\newcommand\vldbtitle{\shorttitle} 
% leave empty if no availability url should be set
\newcommand\vldbavailabilityurl{URL_TO_YOUR_ARTIFACTS}
% whether page numbers should be shown or not, use 'plain' for review versions, 'empty' for camera ready
\newcommand\vldbpagestyle{plain} 

\usepackage{subfigure}
\usepackage{booktabs} % for better-looking tables
\usepackage{wasysym}
\usepackage{pifont}
\usepackage{tikz}
\newcommand{\filledcirc}[2][black]{\tikz[baseline=-0.8ex]\node[draw,circle,fill=#1,inner sep=0pt,minimum size=2mm] {\textcolor{white}{#2}};}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*}


\begin{document}
%  SepHash: Seperate Segment Structure Enabled Write-Optimized Hash Index for Disaggregated Memory
% SepHash: A Write-Optimized Hash Index with Seperate Segment Structure for Disaggregated Memory
% SepHash: A Write-Optimized Hash Index On Disaggregated Memory
\title{SepHash: Separate Segment Structure Enabled Write-Optimized Hash Index for Disaggregated Memory}


%%
%% The "author" command and its associated commands are used to define the authors and their affiliations.
% \author{Xinhao Min Kai Lu Pengyu Liu Yiwen Zhang} 
% \affiliation{%
%   \institution{Huazhong University of Science and Technology}
%   \streetaddress{}
%   \city{}
%   \state{}
%   \postcode{}
% }
% \email{}

% \author{Kai Lu}
% \affiliation{%
%   \institution{WNLO,Huazhong University of Science and Technology}
%   \streetaddress{}
%   \city{}
%   \country{}
% }
% \email{}


%%
%% The abstract is a short summary of the work to be presented in the
%% article.
% 分离内存将传统的庞大服务器划分成各种专用的组件，包括计算节点和内存节点，以获得高资源利用率、独立的扩展性和故障隔离.哈希表是一类提供高单点操作性能的基础结构.然而当前存在的哈希索引结构因分布式通信，扩展开销巨大，无法高效处理写密集负载；此外，由于复杂的并发控制和变长键值的读放大，导致了巨大的索引访问开销。
% 为了提供DM环境下的，高效读写访问性能，这篇文章提出了 A Write-Optimized Hash Index for Disaggregated Memory ,called separate,
% 1）为了高效完成索引扩展，separate提出了一个两层的动态hash结构和Depth Inline Entry，大大降低了扩展带来的带宽消耗
% 2）为了降低并发控制开销，separate利用RTT-less Concurrency Control，避免插入操作不必要互斥和检查开销
% 3) 为了平衡读性能，separate设计了Directory Cache和FPTable Cache来减少读操作.
% 测试表明，separate在相比于最新的工作，在保持读性能相近的情况下，写性能提高XX倍。

\begin{abstract}
Disaggregated Memory (DM) separates traditional servers into  dedicated components,including compute nodes and memory nodes, to achieve high resource utilization, independent scalability,and failure isolation. The hash table is a fundamental structure which provides fast point operations. However, under disaggregated architecture, existing hash indexes suffer from high resizing overhead, including entry-based move strategies and out-of-place key-value read amplification, making them inefficient for processing write-intensive workloads. Moreover, concurrency control schemes based on locks or repeated reads introduce additional read and write operations, thus increasing write latency. Furthermore, designing write-optimized index structures while ensuring excellent read performance poses new challenges for index design.
% Furthermore, designing DM-friendly concurrency control strategies and balancing read performance pose new challenges for index structure design.
% 最后一句的Furthermore, designing DM-friendly concurrency control strategies and balancing read performance pose new challenges for index structure design.会不会太突兀了，能怎么修改一下吗。需要增加具体解释吗
% 此外，基于锁或重读的并发控制方案，引入额外的读写操作，增加了写操作的延迟。同时在设计写优化索引结构的同时兼顾优异的读性能，对索引设计提出了新挑战。

To provide efficient indexing services in disaggregated memory scenarios, this paper proposes a write-optimized hash index called SepHash. To efficiently handle index resizing, SepHash proposes a two-level separate segment structure that significantly reduces the bandwidth consumption of resizing.
% To efficiently handle index resizing, SepHash proposes a two-level dynamic hash structure that significantly reduces the bandwidth consumption of resizing. % 要在这里引入Segment Structure的概念吗
To reducing the latency overhead caused by concurrency control, SepHash employs a RTT-less concurrency control scheme to eliminate unnecessary mutual exclusion and check overhead during insert operations. To balance read performance, SepHash introduces space-optimized cache and filter structures to accelerate read operations and minimize unnecessary read access. Our evaluation shows that, separate outperforms state-of-the-art distributed in-memory hash indexes by achieving a 3.3x improvement in write performance while maintaining comparable read performance.
\end{abstract}

\maketitle

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\pagestyle{\vldbpagestyle}
\begingroup\small\noindent\raggedright\textbf{PVLDB Reference Format:}\\
\vldbauthors. \vldbtitle. PVLDB, \vldbvolume(\vldbissue): \vldbpages, \vldbyear.\\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi}
\endgroup
\begingroup
\renewcommand\thefootnote{}\footnote{\noindent
This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit \url{https://creativecommons.org/licenses/by-nc-nd/4.0/} to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing \href{mailto:info@vldb.org}{info@vldb.org}. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. \\
\raggedright Proceedings of the VLDB Endowment, Vol. \vldbvolume, No. \vldbissue\ %
ISSN 2150-8097. \\
\href{https://doi.org/\vldbdoi}{doi:\vldbdoi} \\
}\addtocounter{footnote}{-1}\endgroup
%%% VLDB block end %%%

%%% do not modify the following VLDB block %%
%%% VLDB block start %%%
\ifdefempty{\vldbavailabilityurl}{}{
\vspace{.3cm}
\begingroup\small\noindent\raggedright\textbf{PVLDB Artifact Availability:}\\
The source code, data, and/or other artifacts have been made available at \url{https://github.com/minxinhao/MyIndex}.
\endgroup
}
%%% VLDB block end %%%

\section{Introduction}
Recently, disaggregated memory architecture has received widespread attention from both academia[XXX] and industry[XXX] due to its high resource utilization, scalability, and failure isolation advantages. 
Disaggregated memory (DM)
% decomposes the large homogeneous servers in traditional data centers into two separate pools: a computation pool and a memory pool. 
decouples compute (CPU) and memory resources from traditional monolithic servers to form independent resource pools. 
The computation pool contains rich CPU resources but minimal memory resources, while the memory pool contains large amounts of memory but weak computational capabilities. 
The computation pool accesses the memory pool through RDMA-capable networks such as InfiniBand, RoCE, and Omnipath[xxx], which offer salient features including remote CPU bypass[xxx], low latency, and high bandwidth[xxx].

Distributed hash indexes are widely used for high-performance data indexing services such as databases[xxx], KV-stores[XXX], memory pools[xxx]. However, due to the near-zero computation power of memory pools, traditional solutions[xxx] based on two-sided RDMA operations cannot be efficiently applied to disaggregated memory architectures. Prior work, e.g., RACE\cite{race}, foucsed on designing hash structures that are friendly to one-sided RDMA access. Nevertheless, when facing write-intensive workloads, the write performance suffers severely due to the significant bandwidth consumption caused by resizing operations. One common way to optimize the write performance of index structures is to introduce leveling index structures that optimize data movement, such as CLevel[x] and Plush[x].  However, their design does not cater to disaggregated memory architectures, leading to frequent network communication induced by a large number of small-sized reads and writes. Furthermore, the multi-level index structure significantly compromises indexing read performance. Directly transplanting these solutions is inadequate to attain the desired performance. 
% Another genre of write-optimized hash indexing employs hierarchical structures to mitigate the overhead of resizing, such as CLevel[x] and Plush[x].
% 重了
In a nutshell, it is non-trivial to design an efficient hash index for disaggregated memory due to the following challenges:

%总的来说，现有的分离内存的hash结构遭受读性能下降，而写优化的hash方案并不适用于分离内存。本文致力于为分离内存重新设计高性能的hash索引，但面临以下挑战：
% In a nutshell, the existing hash index for disaggregated memory suffers from low write performance, while the write-optimized hash indexes are not suitable for disaggregated memory. This paper is devoted to redesigning high-performance hash indexes for disaggregated memory, but faces the following challenges:

\textbf{1) Significant resizing overhead}.  The computational capabilities of memory nodes are limited, and transferring resizing operations to clients results in substantial network overhead. Existing write optimization methods, such as extendible hash and multi-level structure, concentrate on minimizing the aggregate data volume moved during each scaling operation. Nonetheless, they utilize a split transfer strategy that relocates data at the granularity of individual entries, resulting in a significant consumption of network bandwidth and becoming a performance bottleneck. Simultaneously, in the context of variable-length key-value payloads, transferring each entry requires accessing the original KV, resulting in a significant increase in read amplification.

\textbf{2) Concurrency control (CC) overhead}.
% \textbf{2) Concurrency control (CC) strategies}. 
Concurrent access operations to the index require robust mechanisms for ensuring data consistency. When multiple clients access an index concurrently, they may read or write the same key at the same time. To avoid issues such as insert-miss and duplicate key [xxxx], current approaches generally adopt re-read or lock-based concurrency control strategies. However, both methods incur additional communication round-trip time (RTT) and increase the latency of write operations.

\textbf{3) Sacrificing read performance}.
% \textbf{3) Balancing read performance}. 
Achieving optimal write performance in distributed key-value stores often requires compromising the orderliness of the data layout, which can negatively impact read performance. For instance, a hierarchical hash structure can result in key distribution across multiple levels, leading to an increased number of read operations. Furthermore, considering the limited memory resources on the computational nodes, restricted RDMA access interface and comminication overhead, optimizing read performance faces greater limitations.
% Furthermore, considering the limited memory resources on the computational side and the network communication overhead under disaggregated memory architecture, optimizing read performance faces greater limitations.

In this paper, we propose the SepHash, a write-optimized hash index for disaggregated memory. To mitigate the aforementioned challenges, we have introduced a set of innovative techniques, including separate segment structure for efficient resize operation, an RTT-less concurrency control mechanism for reduced index access overhead, and space-efficient cache and filter structures for accelerating read performance. 
% These techniques have been devised to enhance write performance whilst upholding high levels of read performance.
With these techniques, SepHash can deliver high performance for both read and write operations.


\textbf{Separate segment structure}. To mitigate the overhead of resizing, SepHash incorporates the leveling structure into the extendible hash and designs a two-level separate segment structure. 
The separate segment structure includes a \textit{CurLevel} optimized for dynamic writing and a static \textit{MainLevel} optimized for reading, with each level comprising multiple segments. 
The data flow between segments is entirely batch-oriented, avoiding individual entry movement operations. Moreover, SepHash introduces an Depth Inline Entry structure, which significantly reduces the need to access the original KV pairs during the resizing process.

\textbf{RTT-less concurrency control}. To mitigate the overhead of concurrency control, SepHash leverages append write and coroutine technology to design an RTT-less concurrency control strategy. This strategy allows for highly concurrent access to the index structure, without the need for additional locking operations or the overhead of re-reading and checking. Moreover, all write operations can return immediately after inserting the pointer of the key-value pair, without waiting for the completion of subsequent update operations, significantly reducing write latency.
% reducing the latency of write operations.


\textbf{Space-efficient cache and filter}. To balance the read performance, SepHash introduces fingerprint (fp) filter on the memory side and delicate client cache on the computing side. The fp filter reduces unnecessary access to \textit{CurLevel} during the read proccess, while the client cache is used to reduce the number of remote memory accesses and the granularity of access to the segments, thereby minimizing additional read overhead caused by the leveling structure. In addition, considering the overall space utilization of the index and the limited storage resources on the computing side, more compact structures are used for both the fp filter and client cache.

We conducted a comprehensive performance evaluation of SepHash using micro-bench[x] and YCSB[x] workloads. Our experiments demonstrate that, under write-intensive workloads, SepHash significantly improves write performance by 3.3 times compared to state-of-the-art hash index. Additionally, under read-intensive workloads, SepHash achieves a 8.1 times improvement in read performance compared to other hierarchical index structures, which is comparable to state-of-the-art indexing approaches. In summary, we have the following contributions:

\begin{itemize}
    \item We conducted a performance analysis of hash indexes on disaggregated memory and identified three factors contributing to poor write performance: the use of entry-based split strategies, read amplification caused by variable-length key-value pairs, and additional round-trip time overhead from concurrent access control strategies.
    \item We designed and implemented SepHash, a write-optimized hash index on disaggregated memory. SepHash uses a two-level dynamic hash structure, an RTT-less concurrent access control strategy, and space-efficient cache and filter structures to achieve significant improvements in write performance while minimizing the impact on read performance.
    \item We conducted a comprehensive evaluation of SepHash's performance, comparing it against state-of-the-art indexes. Our experimental results demonstrate that separate outperforms these indexes in terms of write performance while achieving a balanced performance trade-off between read and write operations.
\end{itemize}


% 半勾 $\checkmark\hspace{-1.6mm}\smallsetminus$
\begin{table}[tbp]
\centering
\caption{The comparison of SepHash and State-of-the-Art Write-Optimized Hash. (In this table, “$\times$” indicates a bad performance, “$\checkmark$” indicates a good performance and “-” indicates a moderate performance in the corresponding metrics.)}
\label{tab:comparison}
\begin{tabular}{@{}lccccc@{}}
\toprule
 & RACE & CLevel & Plush & SepHash \\
\midrule
Resize-Overhead & $\times$ & - & - & $\checkmark$ \\
Concurrency-Control  & - & $\times$ & $\times$ & $\checkmark$ \\
Read Performance & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ \\
Support DM & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Background And Motivation}

\subsection{Disaggregated Memory}

Disaggregated Memory (DM) architectures segregate different types of resources into separate pools, such as compute and memory pools, which are interconnected using high-performance RDMA networks. The compute pool comprises multiple CPUs with a small amount of DRAM as local cache, while the memory pool consists of a large number of memory resources with weak computing capabilities for handling communication and memory management operations. RDMA networks allow compute nodes to bypass remote CPUs and directly access remote memory, providing RDMA Read, Write, and ATOMIC operations interfaces,e.g.,compare-and-swap(CAS) and fetch-and-add(FAA). RDMA operations are based on the post-poll mechanism, where the user initiates data transfer by posting a work request to the network adapter, which then asynchronously writes data to or reads data from remote memory. During the data transfer process, the user polls the completion queue to check if the operation is complete. From the perspective of the latency overhead of the process, the network asynchronous write operation takes up the majority of the latency overhead, making it suitable for optimization using coroutines. 
% 一般来说，RDMA提供的读写访问接口如传统的

\subsection{Existing hash Index On Disaggregated Memory}

% 需要加这个三级标题吗
\subsubsection{RACE}
\
% To reduce unnecessary reads during lookup operations, each entry in RACE caches the fingerprint information of the key.

RACE[x] is currently the only hash index structure designed for disaggregated memory. RACE uses the extendible hash scheme, which includes a series of segments composed of bucket arrays and a directory that stores pointers to segments. Each bucket contains multiple entries that store only the address of the KV data, rather than directly storing it, to enable support for variable-length KV payloads. To reduce data movement due to hash collisions, each KV pair has two candidate buckets in each segment, and adjacent buckets share a common overflow bucket. Additionally, RACE caches the directory on the client side to minimize the additional RDMA read operations caused by the directory. For write operations, RACE first calculates two candidate buckets in the given segment decided by hash value and retrieves them from the remote server based on the segment address in the local directory cache. Then, the client scans the local buckets to find an empty entryand employs a cas instructions to write the pointer to the key-value pair into it. Finally, since all segment access in RACE is lock-free, the client must re-read the two buckets and scan them again to check for any concurrent threads that may have written the same key. For read operations, RACE also locates the two bucket addresses on the remote server based on the local directory and reads the buckets into the local memory. And then matches the fingerprint information cached in each entry to identify the candidate entries for given key and reads the actual KV data. 

\subsubsection{Performance Analysis}
\

RACE demonstrates excellent performance under read-intensive workloads due to its RDMA-conscious subtable structure, which enables read operations to be completed with just one doorbell-triggered read of two buckets and the use of fingerprint in the entry structure eliminates the need to read each KV pairs in corresponding buckets. Moreover, for infrequent hash collisions, RACE provides ample collision resolution space through its multi-way bucket structure, the overflow bucket shared by two main buckets, and the two-choice bucket, all without requiring additional RDMA access. 

However, in the context of write-intensive workloads, the likelihood of hash collisions increases substantially. When an insert operation cannot find any empty entry in its primary buckets and overflow buckets, a split operation is triggered. To avoid concurrency conflicts, RACE must lock the subtable being resized, create a new subtable, and update the corresponding directory entry. RACE migrates data in units of entries by reading the original KV data, calculating its hash value, and determining its destination after the split. Each migrated entry is written to its target segment using CAS operations and replaced with an empty entry in the old segment using another CAS operation. This process consumes a significant amount of network bandwidth and locks the insert operation that triggered the split for a prolonged period.
% 同时每一个kv触发一个RDMA请求，极大的消耗了网卡带宽，读放大？

We conducted performance tests on the insertion capability of RACE using mirco-bench[x]. By manipulating the initial size of RACE's hash table, we tested the performance of RACE in inserting 100 million keys from scratch with and without split. As shown in Figure 1a, the test results reveal that in the presence of resize operations, the insert performance is decreased by 50\% compared to the ideal scenario, indicating that split operations account for a considerable portion of the index overhead.

\begin{figure}[tbp]
\centering
\subfigure[Insert Throughput of RACE under Write-Intensive Workloads]
{
    \begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.5]{figures/race-throughput.png}
    \end{minipage}
}
\subfigure[Breakdown of Insert Latency in RACE]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.5]{figures/race-break-down.png}
    \end{minipage}
}
% ReadBuc,代表insert中读取bucket，CasSlot代表将kv pointer写入empty entry,ReRead重读buckets。
% ReadSeg,代表Split过程中读取整个Segment，ReadKV代表读取每个entry对应的kv,MoveEntry搬迁entry到new segment和擦除old segment中的entry.

\caption{Performance of RACE on dissaggregated memory. }
\end{figure}


\subsubsection{Latency breakdown}
\

We conducted a more granular decomposition of the insert latency overhead in RACE, as depicted in Figure 1b. Specifically, we identified the three RTTs in the insert process,  including reading the corresponding bucket (ReadBuc), using a cas operation to write the KV pointer into an empty entry in the bucket (CasSlot), and re-reading the bucket to check if the write was successful (ReRead). Additionally, we decomposed the split process into ReadSeg, ReadKV, and MoveEntry, which respectively correspond to reading each bucket in the split segment, retrieving the corresponding KV for each entry, and erasing/writing the entry that needs to be moved. As expected, the breakdown results reveal that in the presence of split operations, the overhead introduced by split accounts for half (51\%) of the total latency. Notably, the overhead due to erasing/writing entries represents the largest portion of the index overhead, comprising 34\% of the total. Reading the corresponding KV for each entry also contributes significantly to the split operation overhead, at 30\%. Additionally, the concurrent control policy results in ReRead operations, which account for one-third of the overhead in the normal insert process. 

To sum up, the index structure and concurrent control policy adopted by RACE fail to meet the performance demands of write-intensive workloads. The entry-based move strategy and out-of-place KV strategy result in substantial bandwidth consumption during resizing. And the concurrent control policy introduces a considerable amount of additional overhead that cannot be overlooked.

\subsection{Hierarchical hash for Write Optimization}

In addition to the Extendible Hash, hierarchical hash structures such as CLevel[X] and Plush[x] represent another class of optimizations for improving write performance of hash indexes. By enabling the presence of multiple levels of hash tables, these structures transform static hash tables into dynamic hash tables that can be resized to accommodate write-intensive workloads.

\textbf{CLevel:} CLevel[x] consists of multiple levels of hash tables, with the size of each hash table increasing gradually from bottom to top. Each level's hash table is an array composed of buckets, and all levels are linked together via an adjacent linked list. For each insertion operation, the client traverses each level of the hash table from bottom to top in search of an empty slot and clears any encountered duplicate keys. The client then writes the data to the empty slot closest to the top level. For read operations, the client also traverse each level of the hash table from bottom to top in search of a matching key. And to prevent duplicate keys, the client must continue searching until the top level is reached, even if a matching key is found in an intermediate level. For resize operations, the user thread creates a hash table with twice the size of the top level, inserts it into the adjacent linked list using a cas instruction. The background resize thread migrates data from the lowest level to the newly allocated hash table at the top level until there are only two levels of hash tables remaining.

\textbf{Plush:} Plush incorporates the principles of the Log-Structured Merge (LSM) tree into hash tables. Plush is composed of multiple levels of hash tables, with each level being implemented as a fixed-size Extendible Hash Table. Similar to the LSM tree, the levels of Plush form a pyramid structure with increasing size from top to bottom. For write operations, data is first written to the smallest hash table at the top level. To avoid concurrent write access from other threads, Plush uses locks to protect the segment corresponding to the key from concurrent writes.For read operations, Plush also performs a top-down traversal to search for the key-value pair. However, unlike CLevel, Plush can return the result directly after finding a match for the key since the most recent key is only present in the higher levels. And to reduce the number of unnecessary read operations on segments, Plush utilizes Bloom filters for each segment. For resize operations, when a segment in the top level is full, Plush writes it to the corresponding two segments in the next level below, similar to the Extendible hash scheme. If the segment size reaches its limit, a recursive expansion operation is triggered downwards.

\textbf{Performance Analysis:} The advantage of a hierarchical structure is that it enables batch moves of data during resizing, and allowing resize operations to be performed in the background, which reduces the impact on foreground operations. However both CLevel and Plush are designed specifically for persistent memory (PM) storage media, taking into consideration aspects such as fault recovery and persistence of the index. Moreover, due to the low access latency of local NVM, these index systems involve a significant amount of small-grained read and write access to the index metadata, which can result in significant communication overheads on disaggregated memory architecture. Indeed, the hierarchical structure can increase the number of levels that need to be traversed during a lookup operation, even when optimized with techniques like Bloom filters. As a result, read performance can still be significantly impacted, as reported in performance evaluations in Section 4.3. 

In summary, the use of a hierarchical structure can lead to improved write performance of hash table. However, achieving optimal performance requires considering additional factors, such as concurrency control, read amplification, and erase overhead. As shown in Figure 1a, we applied a similar batch-move approach to RACE, which improved write performance, but there is still significant room for improvement to reach optimal performance. Meanwhile, achieving high read performance in a hierarchical structure is a valuable challenge that needs to be addressed.



\section{SepHash}

\subsection{Overview}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.4\textwidth]{figures/separate-overview.png}
  \caption{The overall architecture of SepHash on disaggregated memory. }
  \label{fig:overview}
\end{figure}

This paper proposes Seph, a write-optimized hash index on disaggregated memory, designed with three core objectives: \textit{low resize overhead}, \textit{low write latency}, and \textit{balanced read performance}. Figure \ref{fig:overview} shows the overall structure of SepHash. The main index structure of SepHash is stored in the memory pool and comprises two levels of hash tables: CurLevel and MainLevel. Each level is organized using an extendible hashing scheme based on the least significant bits of the hash key and indexed using a global directory. Each segment in the CurLevel(CurSegment) corresponds to a segment in the MainLevel(MainSegment), consisting of an array of entries. In the compute pool, clients maintain a cache to accelerate access to the index, and use pure one-sided RDMA verbs to perform read and write operations on the index structure in the memory pool. In section \ref{sec:index_struture}, we present the detailed structure of the Separate Segment Structure and its index resizing method, which supports high read and write performance. Data flow between segments is batched to avoid excessive bandwidth consumption during resizing. In section \ref{sec:rttless-cc}, we introduce a RTT-less concurrency control strategy that reduces write latency to 2 RTTs by optimizing the write logic and utilizing coroutine technology. Finally, in section \ref{sec:cache_filter}, we present a cache and filter structure designed to optimize read performance, which has a smaller space overhead.

\subsection{The SepHash Index Structure}
\label{sec:index_struture}

\subsubsection{Separate Segment Struture}
\

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/SegmentStructure.png}
  \caption{SegmentStructure. }
  \label{fig:segmentstructure}
\end{figure}

As previously mentioned, the current entry-based move scheme introduces significant resizing overhead in hash index structures. Additionally, as analyzed in section 2.2, there is also the issue of read amplification caused by out-of-place kv, the cost of erasing old entries, and the significant performance degradation on read operation caused by hierarchical structures. Furthermore, for hash index structures, space utilization is an important metric that must be considered. In order to reduce index resize overhead, maintain high space utilization, and ensure good read performance, we have designed a sophisticated separate segment structure, which includes four major design choices:

\textbf{1) Dual Level Structure.} The hierarchical structure allows for efficient batch-entry move resizing, reducing the overhead of index resizing. However, using excessively high-level hash tables would require read operations to access more levels, compromising read performance, which is a critical advantage of hash indexing. To balance optimizing index resize overhead and sustaining good read performance, we developed a two-level hash table structure. The structure comprises a CurLevel that accepts dynamic writes and a relatively static MainLevel, each organized using the extendible hash method based on the hash suffix. The MainSegment has a size limit several times larger than that of the CurSegment (e.g., 16, 64). All write operations are first written to CurLevel, and most read operations hit on MainLevel. Data flows from CurLevel to MainLevel and splits within MainLevel in batches.

\textbf{2) Shared CurSegment.} According to [XXXX], the associativity strategy can provide sufficient collision space to avoid frequent hash resize operations when collisions occur and increase the space utilization of the hash index by allowing more KVs to share the same hash. To this end, we have designed a big shared cursegment that allows all kv pairs within the same segment to share the same suffix. In contrast to previous work that utilized smaller-granularity buckets as the unit of hash collisions within segments, the big shared cursegment drastically reduces the frequency of hash resizing caused by collisions. Furthermore, by eliminating metadata in each bucket [XX], shared segments achieves higher space utilization. 
And when combined with the rtt-less concurrency control (\S 3.3) and the client cache (\S 3.4), it is possible to not only avoid the read amplification caused by big segment size, but also achieve lower write latency.
% And when combined with the rtt-less concurrency control (\S 3.3) and the client cache (\S 3.4), it can provide lower write latency.

\textbf{3) Sorted MainSegment.} Mitigating the performance degradation caused by hierarchical structures during reads is challenging. Traditional approaches such as LSM-Tree and Plush perform a level-by-level search and store membership query data structures such as bloom filters for bottom tables. However, we argue that storing bloom filters for MainSegments would consume significant space and have limited acceleration effects on the overall read process, especially on disaggregated memory, as the majority of keys hit within the MainSegment. On the contrary, we added filters to the Segments of the smaller CurLevel, and adopted a sorted structure for the MainSegment, in which all entries are sorted according to fingerprints. We use the number of entries corresponding to each fingerprints to construct a fp table (\S 3.4). With sorted MainSegment and fp table, each key only needs to read a small piece of entries in MainSegment according to fp when performing read operations, which greatly reduces the read amplification of read operations.
% By caching this in the Compute pool, during read operations, each key only needs to read a small segment of entry slots corresponding to its fp.

\textbf{4) Depth Inline Entry.} The out-of-place kv strategy results in significant read amplification overhead during resizing, even when entries are batch moved. To address this, we introduce the Depth Inline Entry structure, which pre-writes four bits of depth information into each entry during insertion. This enables efficient determination of each kv's location during resizing without repeatedly reading kv pairs. Moreover, compared to directly storing the complete kv hash value for each entry, the depth inline entry structure significantly saves space while achieving similar scalability performance. Each entry also contains a 1-bit sign bit, allowing for atomic batch clearing of a CurSegment after resizing by flipping its sign bit.

Combining all the design elements, the structure of the Separate Segment is shown in Figure 3. CurSegment and MainSegment both consist of a contiguous array of entries. The entries in CurSegment share a common suffix and are unordered, whereas the entries in MainSegment are sorted based on their fingerprint values. Each entry includes a 1-bit sign indicating its free status, a 3-bit length indicating the length of the kv block after alignment to 64 bytes\footnote{The proposed approach provides a maximum size limit of 512 bytes per kv block, which is consistent with most kv index. Additionally, more contents can be expanded using linked blocks.}, a 48-bit offset pointing to the specific kv data, a 4-bit depth info, and two 8-bit fingerprints fp and fp2. The fp2 is used to further filter unnecessary accesses to kv pairs when accessing Sorted MainSegment. The CurSegment header includes a sign bit, local depth information of 7 bits, a pointer to the corresponding MainSegment and its length, and a BitMap for access filtering. 

The Separate Segment Structure efficiently supports disaggregated memory and offers the following advantages:

\underline{\textit{Less entry-move overhead}}: By using batch processing to organize data flow between segments, the bandwidth consumption incurred by moving entries. And the sign bit-based state management mechanism avoids the need to clear outdated entries individually.
% And the state management mechanism based on the sign bit eliminates the need to clear outdated entries one by one.

\underline{\textit{Less read amplification}}: The pre-storing information of subsequent split transfers in the entry, avoids the need to read the original kv pairs during the entry transfer process, resulting in a significant reduction in the read amplification caused by out-of-place kv pairs.

\underline{\textit{High space utilization}}: The use of big shared segments reduces the index expansion caused by hash collisions, thereby improving the overall space utilization of the index. And the compact entry structure, filter, etc. minimizes additional space requirements.

\underline{\textit{Balanced read performance}}: Using a two-level hash table structure minimizes the performance degradation caused by hierarchical structures , while using a sorted main segment speeds up access to the MainLevel with the most read hits.

\subsubsection{Sement Merge and Split}
\

% In order to minimize the RDMA network communication during the resizing process, all read and write operations on segments are batched. As a result, both segment merge and split can be completed within 5 RTTs, disregarding the reading of key-value pairs.

Based on the dual-level Separate Segment Structure, all data flows between segments are batched. All write operations are first accumulated in the CurSegment. When all the empty entries in CurSegment are occupied, a Segment Merge is triggered to flush all the data into the MainSegment. When the data in the MainSegment accumulates to its limit, a Split operation is triggered to divide the MainSegment into two smaller MainSegments. Through batched read/write of segments, depth information cached in the entry, and batched atomic clearing for old entries, both segment merge and split can be completed within 5 RTTs.


\textbf{Segment Merge:} When all entries in CurSegment are filled, a merge operation is triggerefd to write the data into the MainLevel and empty all entries in CurSegment. First, \filledcirc[black]{1}
 the CurSegment is read into the client's memory and the entries are sorted according to their fp.  For entries with matching fp and fp2, the corresponding kv pairs are retrieved and compared locally to remove duplicate keys. Then, \filledcirc[black]{2} the corresponding MainSegment is read into the client memory and merged with the sorted and de-duplicated CurSegment, which also includes merging of duplicate keys. The resulting merged entry array becomes the new MainSegment if it hasn't exceeded the set size limit. Subsequently, \filledcirc[black]{3} the new MainSegment is written back to the remote memory, \filledcirc[black]{4} followed by updating the pointer in the CurSegment to the MainSegment. Finally, \filledcirc[black]{5} the CurSegment is atomically cleared to empty entries by flipping the sign bit.

Figure 4a illustrates an example of Segment Merge. When sorting the CurSegment, two entries with fp equal to 8 are matched and found to be duplicates, with only the latter entry being retained. During the merge operation, the entry with fp equal to 5 in the CurSegment updates the old data in the MainSegment, whereas the entry with fp equal to 8 in the CurSegment is simply appended to the MainSegment since it does not correspond to the same kv in the MainSegment. The entry with fp equal to 4, which has no duplicate entries, is added directly. In practice, since both fp and fp2 are used for matching, the need to read kv pairs is very rare.

\textbf{Segment Split:} If the size of the newly generated MainSegment exceeds the set limit during a segment Merge, it needs to be further split. The segmentation principle is consistent with that of extendible hash [xxx], where each entry in the segment is divided into two roughly equal new Segments based on the local depth's bit of hash values. The pointers in the directory that point to segments are updated by comparing the global depth of the directory with the local depth of the segment being split. Based on the pre-stored depth information in the entries and the local depth of the current segment, the segment to which an entry belongs after splitting can be determined without reading actual kv pairs. 

As shown in Figure 4b, the local depth of the old MainSegment is 4. According to the depth information that is updated every 4 splits, the lowest bit of depth information in the entry is currently used. Thus, all entries in the old MainSegment with the lowest depth bit of 0 are split to form a new MainSegment 00, while all entries with the lowest depth bit of 1 are split to form a new MainSegment 01. In the next round of splitting on segments 00 and 01, the first bit of the depth information is used for determination. By reducing the need to access the original key-value pairs, this approach greatly reduces network bandwidth consumption.


\begin{figure}[tbp]
\centering
\subfigure[Segment Merge]
{
    \begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/Segment_Merge.png}
    \end{minipage}
}
\subfigure[Segment Split]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/Segment_SPlit.png}
    \end{minipage}
}
\caption{Segment Merge and Split}
\end{figure}



\subsection{RTT-less Concurrency Control}
\label{sec:rttless-cc}

Traditional concurrency control solutions, including lock-based and lock-free schemes, both suffer from the problem of introducing additional RTT on disaggregated memory. As shown in Figure 5, lock-based solutions require additional steps of acquiring and releasing locks, while re-read based solutions require additional steps of re-reading buckets after write operations are completed, both resulting in additional network communication and increased write operation latency. In order to optimize the write operation latency, we propose an RTT-less concurrency control solution using append write and coroutine technology.



\textbf{Append Write:} In LSM and other hierarchical storage structures, the use of append write strategy can convert random writes to storage devices into sequential writes. However, we have observed that append write strategy also significantly simplifies the concurrent logic of write operations. Under append write strategy, all data is directly appended to the current segment, and only one client can successfully compete for the first empty entry at any given time, eliminating the need to lock the segment. For duplicate keys inserted into the same segment, since the latest key only appears at the end of the segment, search operations can easily obtain consistent results. The write strategy using append write can avoid the additional RTT caused by locking operations and re-reading checks.

\textbf{Zero-wait Write:} Coroutines enable concurrent execution of multiple tasks within a single thread, which can significantly reduce the latency of asynchronous I/O. As mentioned in Section 2.1, RDMA requests are divided into two stages: post and wait-poll. By using coroutines, we have implemented a zero-wait write approach, which enables the immediate return of a write operation after submitting an RDMA request (which only requires accessing local memory), thus reducing network overhead. Separate Segment Structure introduces a series of additional structures such as fp2, fp table, etc. in order to balance read performance, and the update operation of these data loses the latency advantage brought by append write. By using zero-free write, we hide the additional updates to fp2 and fp filter in coroutines scheduling.

The process of RTT-less Concurrency Control is illustrated in Figure 5c. In the first RTT, the client reads the metadata and entries of the corresponding CurSegment from remote memory. In the second RTT, the client selects the first empty slot based on the corresponding sign bit in the Segment header and occupies it using an RDMA cas operation. If the cas operation succeeds, the main process of the write operation is completed, and subsequent updates to fp2 and fp bitmap are performed using zero-wait write. The client returns immediately after posting the RDMA request. By using append write strategy and zero-wait write, the latency of insert operations is reduced to 2 RTTs.


% \begin{figure}[t]
% \centering
% \begin{minipage}[b]{0.45\linewidth}
%   \centering
%   \includegraphics[scale=0.2]{figures/lock-based-cc.png}
%   \caption*{(a) Lock Based CC}
%   \label{fig:image_a}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.45\linewidth}
%   \centering
%   \includegraphics[scale=0.2]{figures/re-read-cc.png}
%   \caption*{(b) Re-Read Based CC}
%   \label{fig:image_b}
% \end{minipage}

% \vspace{0.5cm}

% \begin{minipage}[b]{1\linewidth}
%   \centering
%   \includegraphics[scale=0.2]{figures/RTT-less-CC.png}
%   \caption*{(c) RTT-less CC}
%   \label{fig:image_c}
% \end{minipage}
% \caption{RTT-less Concurrency Control.}
% \label{fig:image}
% \end{figure}

\begin{figure}[tbp]
\centering
\subfigure[Lock-based CC]
{
    \begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/lock-based-cc.png}
    \end{minipage}
}
\subfigure[Re-Read Based CC]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/re-read-cc.png}
    \end{minipage}
}
\subfigure[RTT-less CC]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/RTT-less-CC.png}
    \end{minipage}
}
\caption{RTT-less Concurrency Control.}
\end{figure}


\subsection{Filter and Cache}
\label{sec:cache_filter}

% The Separate Segment Structure and RTT-less CC provide many advantages, such as batch-splitting, high space utilization, and low write latency. However, they also introduce challenges such as read amplification in write operations and longer read processes. To accelerate the read and write processes of separate, we have introduced a compact Filter structure on the Memory side and designed a delicate Cache\footnote{Since all clients on a compute node share a single cache, the space consumption is relatively small, typically only a few tens of megabytes.} structure on the Compute Pool side.

The hierarchical structure inevitably leads to a decrease in read performance, and although the Separate Segment Structure reduces the number of levels that need to be accessed, further optimizations are still needed to improve read performance. As mentioned in Section 3.2, to speed up read access to the hierarchical structure, we designed a filter for CurSegment and a Client Cache on the computation side to reduce unnecessary RDMA access. However, considerint that CurSegment needs to be frequently modified and the limited memory resources on computation side, designing a suitable filter structure for it is not an easy task.


\subsubsection{FP BitMap}
\
% 要不要加个BitMap和其他filter的图

Traditional filter structures such as Bloom filters[x], Quotient filters[x], and Cuckoo filters[x] use multiple hash functions to calculate the position of a given key in the filter array, and then write a 1 to the corresponding bit at that position. This result in multiple network communications on disaggregated memory architecture, which leads to a significant increase in write latency. Additionally, Bloom filter and its variants suffer from high space overhead. Based on the inspiration from using fp for accessing Sorted MainSegment, we have introduced a fp bitmap as a filter for accessing the CurSegment. The fp bitmap records the occurrence of the fp field in the CurSegment and is updated for each insertion operation using RDMA write to modify the corresponding bit at the current kv's fp to 1. During a read access, we first read the fp bitmap in the CurSegment header and only proceed to read the complete CurSegment entry array when the corresponding fp bit is set to 1. Using an fp bitmap as a filter for accessing the CurSegment has two significant advantages:

\textbf{Low space overhead}: Using only the fingerprint to determine set membership, instead of the full key-value pair, can greatly reduce the size of the set that the filter needs to process, thereby reducing the filter's space overhead. Moreover, The fp bitmap has a small space overhead compared to other traditional filter structures, such as Bloom filter and its variants. The fp bitmap only needs to store a bit for each fp field in the CurSegment, resulting in a space overhead of only 1x, while Bloom filter and its variants can have a space overhead of 5x or more[x]. 

\textbf{Minimal modification and read}: Only one modification operation is required for each insertion operation. Bloom filter requires modifying multiple bits corresponding to different hash functions, while Cuckoo filter and Quotient filter require eviction and writing operations for each insertion. In contrast, a fp bitmap as a filter only requires one modification operation using RDMA write to modify the corresponding fp bit. Similarly, during read operations, it is not necessary to read the entire Bloom filter when performing membership checks. Only a single bit needs to be read and checked locally, greatly reducing read amplification.


\subsubsection{Client Cache}
\

Although RTT-CC and filters reduce the number of RDMA operations required for insert and get operation, performing each read and write operation directly on the entire CurSegment and MainSegment would still result in significant read amplification. Furthermore, using the extendible hash approach inevitably introduces directory access and split consistency checks. To reduce the granularity and bandwidth consumption of accessing the CurSegment and MainSegment, as well as to minimize access to the Directory, we implement Directory Cache and FPTable Cache on the client side.
% We save Directory Cache and FPTable Cache on the client side to reduce the granularity and bandwidth consumption of access to the CurSegment and MainSegment, while also reducing access to the Directory.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/CacheSlot.png}
  \caption{CacheSlot. }
  \label{fig:segmentstructure}
\end{figure}

\textbf{Directory Cache.} The directory cache is used to reduce the granularity of accessing CurSegment and to minimize access to the Directory. Specifically, the Directory Cache stores the \textit{offset} of the last accessed free entry for each CurSegment. For each insert operation,  a fixed-length (e.g., 8) contiguous entry array is read from the CurSegment at the offset to search for an empty entry locally. If an empty entry is not found, the offset is increased, and the search continues in the next contiguous entry array using a sliding window approach. This approach eliminates the need to read the entire CurSegment to find an empty entry during write operations. Furthermore, due to the append write mechanism, the offset often hits an empty entry on the first attempt. The directory cache also save pointers to the corresponding CurSegment and MainSegment to reduce the need for remote directory access when accessing the segments. Unlike RACE [x], we use the sign bit and main\_seg\_ptr as consistency checks for the cache, due to the presence of merge operations. Whenever the main\_seg\_ptr read from the CurSegment's header does not match the local main\_seg\_ptr, the sign, offset, and main\_seg\_ptr in the Directory Cache are updated.

\textbf{FPTable Cache}. The FPTable cache is implemented to reduce the granularity of accessing the MainSegment, which is significantly larger than the CurSegment. Since all entries in the MainSegment are sorted by their fingerprint, reading the entire MainSegment would lead to significant read amplification. The most intuitive approach is to store the number of entries corresponding to each fp and calculate the offset of the corresponding fp in the MainSegment during each read access, similar to the offset field in the Directory Cache. However, as illustrated in Figure 6, this uncompact format for the FPTable would impose a significant storage overhead(256 bytes per FPTable). Therefore, we propose a compact delta-based fp table scheme. Due to the uniformity of the hash distribution, the number of entries within each FP interval in the MainSegment is very close (in our testing process, the average data skewness is less than 20\%). Therefore, we only need to reset a new record point when the deviation of entry data exceeds the range estimated according to the average value. This record point represents the new starting offset of the subsequent FPs, which is the delta. As shown in Figure 6, the delta-based reduces the number of required entries to just 32, significantly reducing the storage overhead.
 

% In order to speed up read access to the MainSegment, the MainSegment is organized in a structure sorted by the file pointer (FP). However, directly reading the entire MainSegment would cause significant read amplification. Storing the number of entries for each FP, as shown in Figure 6, would be the most straightforward approach, but it would consume a large amount of storage space (256 bytes per FPTable). This would exacerbate the memory resource constraints on the computing side. Therefore, we propose a compact delta-based FP table scheme. Due to the uniformity of the hash distribution, the number of entries within each FP interval in the MainSegment is very close (in our testing process, the average data skewness is less than 20\%). This is because data skewness only exceeds a certain range, such as 16 entries, after a continuous range of FPs. Therefore, we only need to reset a new record point every certain FP interval when the deviation of entry data exceeds the range estimated according to the average value. This record point represents the new starting offset of the subsequent FPs, which is the delta. As shown in Figure 6, the delta-based scheme can greatly reduce the storage space required for the FP table.



\section{EVALUATION}

% In this section, we test SepHash and other comparison to answer the following questions:

% \begin{itemize}
%     \item How does SepHash perform under write-intensive workloads? How do various techniques employed in SepHash contribute to overall performance?
%     \item How does SepHash's write latency compare to other indexes? Has SepHash achieved achieved the goal of low write latency?
%     \item How does SepHash's read performance compare to other indexes? Has it achieved the goal of balanced read performance?
%     \item How do various optimization techniques impact internal factors such as RTT, read amplification, and split frequency on SepHash?
%     \item How does the space utilization of SepHash compare with other indexes?
%     \item How does different key sizes impact index performance?
% \end{itemize}

\subsection{Experimental Setup}
\

\textbf{Hardware Platform.} We run all experiments on 8 machines, each with two 26-core Intel Xeon Gold 5218R CPUs, 384 GiB DRAM,and one 100Gbps Mellanox ConnectX-5 IB RNIC. Each RNIC is connected to a 100Gbps Mellanox IB switch. One machine is used for emulating the memory pool. To emulate the weak compute power, CPUs in the memory pool are only used for registering memory to RNICs during the initialization stage and do not involve in any requests of hashing indexes. The memory is registered with huge pages to reduce the page translation cache misses of RNICs [13]. The other machine is used for building the compute pool in which each CPU core serves as a client. Since current RNIC hardware does not support remote memory allocation in real time, we enable clients to pre-allocate all memory that future insertion and update requests require, which can also reduce the memory allocation latency in the critical path of request execution.

\textbf{Workloads.} We use micro-benchmarks[x] and YCSB[x] to evalute the performance of different hash indexes. For most tests, we used 16-byte keys and 32-byte values that are representative of key-value stores in real-world workloads [xxx]. We use four YCSB workloads with different read/write ratio as shown in table 2.
% We also tested the impact of different key sizes on performance.
% 暂时不确定要不要测key-size的影响

% \begin{table}[tbp]
% \centering
% \caption{YCSB Workloads}
% \label{tab:workload_ratios}
% \begin{tabular}{|c|c|c|c|}
% \hline
% Workload & Insert & Update & Distribution \\ \hline
% A & 0.5 & 0.5 & zipfian \\ \hline
% B & 0.95 & 0.05 & zipfian \\ \hline
% C & 1.0 & 0.0 & zipfian \\ \hline
% D & 0.95 & 0.05 & latest \\ \hline
% \end{tabular}
% \end{table}
\begin{table}[htbp]
\centering
\caption{Workload Ratios}
\label{tab:workload_ratios}
\begin{tabular}{|c|c|c|c|c|}
\hline
Workload & Read & Update & Insert & Distribution \\ \hline
A & 0.5 & 0.5 & 0 & zipfian \\ \hline
B & 0.95 & 0.05 & 0 & zipfian \\ \hline
C & 1.0 & 0 & 0 & zipfian \\ \hline
D & 0.95 & 0 & 0.05 & latest \\ \hline
\end{tabular}
\end{table}

\textbf{Comparisons.} 
We compared SepHash with three distributed hash indexes. RACE is the only distributed hash designed natively for disaggregated memory, and because RACE is not open-sourced, we implemented it from scratch. Clevel and plush are single-machine hash indexes optimized for write performance using a hierarchical hash structure and we port them to disaggregated memory by replacing persistent memory read/write with RDMA read/write, to verify the performance of hierarchical hash structures on memory disaggregation. For RACE, based on previous work, we used a configuration of 8 entries per bucket to match the granularity of RDMA access. And for all comparisons, we maintained a similar number of initial entries for each hash table, ensuring that the conditions for the first split were comparable. Additionally, clevel used a client as a background thread to execute expansion operations.

% RACE employs 128 buckets per segment, with a total segment size of 12KB. Cluster and clevel used an initial table size of 2048 buckets, while plush used an initial table size of 16 buckets per group and 128 groups. SepHash used a CurSegment size of 1KB and 113 entries, as well as a MainSegment size 16 times that of CurSegment. Additionally, clevel used a client as a background thread to execute expansion operations.


\subsection{Overall Performance}

\subsubsection{Micro-Benchmarks}
\

First, we use micro-benchmarks to test the basic write throughput of SepHash and the compared systems. We insert 100 million kv data into different hash indexes and record their throughput. At the same time, we change the number of clients to test their performance under concurrent writes. When the number of clients is less than 16, they are run on the same client machine. When the data is 32, 64, and 128, they are run on 2, 4, and 8 servers respectively. Each client runs using 1 to 4 coroutines, and the configuration is determined by selecting the number of coroutines that provides the highest performance.

Figure 7 depicts the concurrent throughput of the various indexes under write-intensive workloads. Overall, SepHash exhibits the best performance among all systems, with its write throughput showing good scalability with increasing numbers of concurrent clients. RACE's write performance gradually reaches a bottleneck as the number of concurrent clients increases due to its significant overhead for scaling. Clevel's write and scaling performance is also excellent due to the presence of background scaling threads that allow foreground threads to operate seamlessly. However, due to its longer insertion path, Clevel's write performance lags behind SepHash. Plush, on the other hand, has excellent single-threaded write performance, but its write scaling performance is poor due to the use of lock mechanisms.

% 提高多少多少倍

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/micro_bench_insert.pdf}
  \caption{Mirco-benchmarks throughput. }
  \label{fig:segmentstructure}
\end{figure}

 
\subsubsection{YCSB WorkloadS}
\

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/ycsb_performance.pdf}
  \caption{YCSB Performance. }
  \label{fig:segmentstructure}
\end{figure}

We conducted performance evaluations of different hash indexes under a mixed workload with varying read-write ratios using the YCSB benchmark. To begin, we inserted 10 million key-value pairs into the hash tables, followed by 1 million tests. We employed four distinct workload ratios, as depicted in Table 2. Our results indicate that RACE and SepHash outperformed Plush and Clevel across all tested workloads, exhibiting significantly higher performance. RACE exhibits excellent performance under all workloads since the update operation only involves updating the corresponding pointer after a search and does not require modifying the index structure. Therefore, all workloads for RACE are read-intensive workloads, which benefits from its search-friendly structure. SepHash achieves performance similar to RACE by optimizing read granularity and reducing read amplification through the use of filters and caches. However, Plush and Clevel perform poorly due to their multi-level search strategies, which lead to significant read amplification.

\subsection{Space Overhead Analysis}

% the total size of hash index; average space utiliazation of hash index;
% Space utilization is a critical performance metric for hash indexes. 
% 这里要不要将entry uitlization改成load factor
\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/entry_utilization.pdf}
  \caption{entry-utilization }
  \label{fig:segmentstructure}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/space_utilization.pdf}
  \caption{space-utilization. }
  \label{fig:segmentstructure}
\end{figure}

To compare the space overhead of different indexes, we introduce the concepts of \textit{entry utilization} and \textit{space utilization}. Entry utilization refers to the ratio of the number of entries used to store inserted data to the total number of entries in the index. Space utilization, on the other hand, refers to the ratio of the space used by the entries that point to the actual KV data to the total space used by the index. Furthermore, we focus on the average entry utilization and space utilization of the indexes rather than their maximum values, as they provide a more representative measure of the index's space efficiency over time. We calculate and analyze the entry utilization of each index and the space utilization of the index by recording the entry utilization and space utilization of the index every 1,000 KV pairs inserted, starting from the insertion of 10,000 KV pairs (which is close to the point where the index triggers its first resize operation). For Clevel, we  wait for the background rehash thread to complete before measuring the utilization metrics.

Figure 9 presents the entry utilization and space utilization of the indexes. The experimental results demonstrate that SepHash consistently achieves an entry utilization and space utilization of over 90\%. And as the amount of inserted data increases, SepHash's entry utilization and space utilization further improve, as more valid data accumulates in the MainSegment, which has 100\% entry utilization and minimal metadata overhead. However, for RACE and CLevel, despite having a near-maximum entry utilization of around 90\%, their entry and space utilization sharply decrease after every resize operation, resulting in suboptimal average space utilization. Particularly for CLevel, which undergoes full-table expansion, frequent resize operations lead to an entry utilization rate below 20\%. Due to the metadata present in Directory, Segment, and Bucket, RACE experience a decline in space utilization compared to entry utilization. And Plush, which benefits from a hierarchical accumulation mechanism similar to that of SepHash, exhibits exceptional entry utilization that gradually increases with the quantity of inserted data. However, The reserved Directory space required for each additional layer and the significant number of filters retained by high-level buckets result in a significant decline in space utilization for Plush.



\subsection{In-Depth Analysis}

To analyze the performance of SepHash and validate its optimization design, we decompose the performance gap between RACE and SepHash by applying SepHash's optimization techniques one by one. Figures 11 and 12 depict the optimization results for write and read operations, respectively.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/insert_op_breakdown.pdf}
  \caption{insert-op-breakdown. }
  \label{fig:segmentstructure}
\end{figure}

\textit{Insert Optimization.}
In Figure 11, \textbf{+Separate Segment Structure} represents the application of Separate Segment Structure and its corresponding Segment Merge/Split indexing operations. \textbf{+inline-depth entry} indicates the introduction of inline depth information for entries, while \textbf{+rtt-less cc} represents the incorporation of append write and zero-wait RDMA write optimized by coroutine technology. The introduction of Separate Segment Structure provides comparable performance to RACE under low-concurrency conditions. Under high-concurrency conditions, SepHash leverages the support for Batch Move in Separate Segment Structure to reduce the bandwidth consumption during index resizing, enabling more concurrent operations. This results in a 1.6x performance improvement over RACE with 128 clients. The subsequent introduction of inline-depth entry further reduces the single-point RDMA read required during entry movement, doubling the performance improvement compared to Separate Segment Structure alone. This finding corroborates our analysis of the overhead of RACE expansion in background tests. Finally, the incorporation of rtt-less cc allows SepHash's insert operation to return without waiting for all DMA writes to complete, further enhancing indexing write performance to 3.3x that of RACE.

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/search_op_breakdown.pdf}
  \caption{search-op-breakdown. }
  \label{fig:segmentstructure}
\end{figure}

In Figure 12, \textbf{base} represents the scheme of directly scanning the entire CurSegment and MainSegment for key matching, \textbf{+fptable cache} indicates the introduction of an optimized fptable for the sorted MainSegment, and \textbf{+fp filter} represents the addition of a bitmap filter to filter out unnecessary access to the CurSegment. The Base scheme exhibits performance limitations due to high bandwidth consumption from the large CurSegment and MainSegment sizes, resulting in bandwidth saturation as the number of concurrent clients increases. The adoption of an FpTable Cache approach mitigates this issue by reducing the access granularity for the MainSegment from full-table accesses to accesses corresponding to a single fingerprint (fp), thereby reducing the access granularity by a factor of 1/256. This approach significantly lowers the bandwidth demands on the underlying RDMA networking interface, enabling support for more concurrent read requests. Specifically, after implementing the FpTable Cache, the average SepHash read performance at 128 concurrent accesses exhibited a 22X improvement compared to the Base scheme without the cache. However, each read operation still requires accessing the CurSegment, which is KB in size, limiting further performance improvements. The introduction of fp filter allows negative searches of CurSeg to be avoided by only examining the header of CurSegment. Implementing the fp filter in conjunction with the FpTable Cache resulted in a 2X increase in SepHash's read performance compared to using the FpTable Cache alone, approaching the read performance of RACE..


\subsection{Write latency}

To test the latency of index writes, we measured the average write latency of different indexes during the load phase and run phase under the YCSB workload. To demonstrate the stability of write latency, we also tested the impact of different load data sizes on index write latency. All run phases inserted 10 million kv pairs. 

As shown in Figure 13, SepHash's write latency performance was far superior to other indexes during both the load phase and run phase. Compared to RACE, SepHash achieved up to 73\% lower write latency during the load phase and up to 60\% lower write latency during the run phase. 

RACE's write latency increased significantly with the load data size during the load phase. This is because more and more split operations were triggered, consuming a large amount of network bandwidth while blocking foreground inserts. With the increase of load phase data size, RACE's write latency in the run phase actually decreased, because the load phase expanded the index sufficiently large, reducing the number of splits in the run phase. At a load data size of 10 million, RACE's write latency was lowest, almost 3x lower than at a load data size of 1 million.  In contrast to RACE, SepHash's write latency remained stable during both the Load and Run phases. This is due to SepHash's Separate Segment Structure which distributes index splits evenly across all phases of index operation, avoiding drastic fluctuations in write latency. Plush also exhibited stable write latency during both the Load and Run phases due to its use of hierarchical flushing and extendible splits for index expansion. In contrast, Clevel's insertion method which scans free entries level by level resulted in increasing levels as the amount of inserted data increased, leading to rising write latency in both the Load and Run phases. 

Finally, due to the introduction of the zero-wait write technique, SepHash was able to better pipeline concurrent RDMA read and write requests, resulting in lower latency even after discounting the effects of splits. During the run phase after loading 10 million data, SepHash's write latency was still 30\% lower than RACE's.



\begin{figure}[tbp]
\centering
\subfigure[load-latency]
{
    \begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.5]{figures/load_latency.pdf}
    \end{minipage}
}
\subfigure[run-latency]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.5]{figures/run_latency.pdf}
    \end{minipage}
}
\end{figure}

\subsection{Separate Segment Structure}


\begin{figure}[tbp]
\centering
\subfigure[MainSegSize-Insert]
{
    \begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/MainSegSize-Insert.pdf}
    \end{minipage}
}
\subfigure[MainSegSize-Search]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/MainSegSize-Search.pdf}
    \end{minipage}
}
\end{figure}

\begin{figure}[tbp]
\centering
\subfigure[CurSegSize-Insert]
{
    \begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/CurSegSize-Insert.pdf}
    \end{minipage}
}
\subfigure[CurSegSize-Search]
{
 	\begin{minipage}[]{1\linewidth}
        \centering
        \includegraphics[scale=0.25]{figures/CurSegSize-Search.pdf}
    \end{minipage}
}
\end{figure}

\subsection{Coroutine Optimization}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/plot_times.pdf}
  \caption{coroutine improvement }
  \label{fig:segmentstructure}
\end{figure}

\begin{acks}
 This work was supported by the [...] Research Fund of [...] (Number [...]). Additional funding was provided by [...] and [...]. We also thank [...] for contributing [...].
\end{acks}

\clearpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample}

\end{document}
\endinput
